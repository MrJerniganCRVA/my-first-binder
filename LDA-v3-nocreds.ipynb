{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import tweepy\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment and run this cell\n",
    "#! python -mpip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer = \"ENTER YOUR BEARER TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=bearer, wait_on_rate_limit=True)\n",
    "\n",
    "def get_tweets(client, input_query, n=1000):\n",
    "    \"\"\"\n",
    "    This collects tweets and associated metadata as well as including associating\n",
    "    users metadata with the tweet.\n",
    "    \"\"\"\n",
    "    page_size = 100\n",
    "    pages = n // 100 + 1\n",
    "    i = 0\n",
    "    for tweet_batch in tweepy.Paginator(client.search_recent_tweets, input_query,\n",
    "                                  tweet_fields=[\"created_at\", \"public_metrics\", \"entities\"],\n",
    "                                   expansions=[\"author_id\"],\n",
    "                                  user_fields=[\"username\", \"public_metrics\"],\n",
    "                                  max_results=page_size, limit=pages):\n",
    "        # user data is sent in a package alongside the returned tweets\n",
    "        user_lookup = {u.id: u.data for u in tweet_batch.includes[\"users\"]}\n",
    "        for tweet in tweet_batch.data:\n",
    "            data = tweet.data\n",
    "            # \n",
    "            data[\"author\"] = user_lookup[tweet.author_id]\n",
    "            i += 1\n",
    "            yield tweet\n",
    "            # stop exactly at the nth tweet otherwise the api will return the rest\n",
    "            # of the data from the same page.\n",
    "            if i == n:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Tweets for Virginia Tech, Harvard, and Stanford\n",
    "\n",
    "Please note that we use three different topics in this case just for clarity. You can collect a large set of tweets from Virginia Tech and still be able to do the topic modeling as follows.  This will probably take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Virginia Tech\"\n",
      "processed 100 tweets: saved 100\n",
      "processed 200 tweets: saved 200\n",
      "processed 300 tweets: saved 300\n",
      "processed 400 tweets: saved 400\n",
      "processed 500 tweets: saved 500\n",
      "processed 600 tweets: saved 600\n",
      "processed 700 tweets: saved 700\n",
      "processed 800 tweets: saved 800\n",
      "processed 900 tweets: saved 900\n",
      "processed 1000 tweets: saved 1000\n",
      "Harvard\n",
      "processed 100 tweets: saved 100\n",
      "processed 200 tweets: saved 200\n",
      "processed 300 tweets: saved 300\n",
      "processed 400 tweets: saved 400\n",
      "processed 500 tweets: saved 500\n",
      "processed 600 tweets: saved 600\n",
      "processed 700 tweets: saved 700\n",
      "processed 800 tweets: saved 800\n",
      "processed 900 tweets: saved 900\n",
      "processed 1000 tweets: saved 1000\n",
      "Stanford\n",
      "processed 100 tweets: saved 100\n",
      "processed 200 tweets: saved 200\n",
      "processed 300 tweets: saved 300\n",
      "processed 400 tweets: saved 400\n",
      "processed 500 tweets: saved 500\n",
      "processed 600 tweets: saved 600\n",
      "processed 700 tweets: saved 700\n",
      "processed 800 tweets: saved 800\n",
      "processed 900 tweets: saved 900\n",
      "processed 1000 tweets: saved 1000\n"
     ]
    }
   ],
   "source": [
    "input_queries = ['\"Virginia Tech\"', \"Harvard\", \"Stanford\"]\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "download_tweet_count = 1000\n",
    "seen = {}\n",
    "for input_query in input_queries:\n",
    "    #Download, skipping retweets, look for english\n",
    "    input_query_nort = \"{} -is:retweet lang:en\".format(input_query)\n",
    "    print(input_query)\n",
    "    q_dataset = []\n",
    "    for i, tweet in enumerate(get_tweets(client, input_query_nort)):\n",
    "        data = tweet.data\n",
    "        data[\"topic\"] = input_query\n",
    "        q_dataset += [data]\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"processed {} tweets: saved {}\".format(i + 1, len(q_dataset)))\n",
    "    dataset = pd.concat([dataset, pd.json_normalize(q_dataset)])\n",
    "dataset.to_json(\"tweets-lda.jsonl\", orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>entities.mentions</th>\n",
       "      <th>entities.annotations</th>\n",
       "      <th>public_metrics.retweet_count</th>\n",
       "      <th>public_metrics.reply_count</th>\n",
       "      <th>...</th>\n",
       "      <th>author.id</th>\n",
       "      <th>author.name</th>\n",
       "      <th>author.public_metrics.followers_count</th>\n",
       "      <th>author.public_metrics.following_count</th>\n",
       "      <th>author.public_metrics.tweet_count</th>\n",
       "      <th>author.public_metrics.listed_count</th>\n",
       "      <th>author.username</th>\n",
       "      <th>entities.urls</th>\n",
       "      <th>entities.hashtags</th>\n",
       "      <th>entities.cashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@RSIRob @scs_real He’s at Virginia Tech now. O...</td>\n",
       "      <td>[1637901189865177096]</td>\n",
       "      <td>1637901189865177088</td>\n",
       "      <td>2023-03-20 19:37:29+00:00</td>\n",
       "      <td>1391907070031630336</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "      <td>[{'start': 0, 'end': 7, 'username': 'RSIRob', ...</td>\n",
       "      <td>[{'start': 26, 'end': 38, 'probability': 0.979...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1391907070031630336</td>\n",
       "      <td>Grant Montgomery 2.0 🇺🇸🇮🇱🇯🇵</td>\n",
       "      <td>4508</td>\n",
       "      <td>4510</td>\n",
       "      <td>10791</td>\n",
       "      <td>1</td>\n",
       "      <td>monsgomeric</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virginia Tech women's basketball advances to t...</td>\n",
       "      <td>[1637899898627448833]</td>\n",
       "      <td>1637899898627448832</td>\n",
       "      <td>2023-03-20 19:32:21+00:00</td>\n",
       "      <td>45960001</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 0, 'end': 18, 'probability': 0.8393...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>45960001</td>\n",
       "      <td>Collegiate Times</td>\n",
       "      <td>20332</td>\n",
       "      <td>327</td>\n",
       "      <td>18488</td>\n",
       "      <td>448</td>\n",
       "      <td>CollegiateTimes</td>\n",
       "      <td>[{'start': 112, 'end': 135, 'url': 'https://t....</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The NCAA Women's Basketball Tournament banned ...</td>\n",
       "      <td>[1637899418279059458]</td>\n",
       "      <td>1637899418279059456</td>\n",
       "      <td>2023-03-20 19:30:26+00:00</td>\n",
       "      <td>237286926</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 4, 'end': 37, 'probability': 0.5539...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>237286926</td>\n",
       "      <td>96.3 WROV</td>\n",
       "      <td>896</td>\n",
       "      <td>529</td>\n",
       "      <td>16209</td>\n",
       "      <td>9</td>\n",
       "      <td>ROVRocks</td>\n",
       "      <td>[{'start': 136, 'end': 159, 'url': 'https://t....</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DaVinci Bead Virginia Tech – Jewelry Bracelet ...</td>\n",
       "      <td>[1637899291661598726]</td>\n",
       "      <td>1637899291661598720</td>\n",
       "      <td>2023-03-20 19:29:56+00:00</td>\n",
       "      <td>3238156627</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3238156627</td>\n",
       "      <td>Wear Your Own Techs</td>\n",
       "      <td>3684</td>\n",
       "      <td>3768</td>\n",
       "      <td>139373</td>\n",
       "      <td>36</td>\n",
       "      <td>wyot23</td>\n",
       "      <td>[{'start': 104, 'end': 127, 'url': 'https://t....</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virginia Tech Fans Sing Metallica’s ‘Enter San...</td>\n",
       "      <td>[1637898477525950464]</td>\n",
       "      <td>1637898477525950464</td>\n",
       "      <td>2023-03-20 19:26:42+00:00</td>\n",
       "      <td>43113373</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'start': 0, 'end': 12, 'probability': 0.912,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>43113373</td>\n",
       "      <td>WCYY</td>\n",
       "      <td>3996</td>\n",
       "      <td>679</td>\n",
       "      <td>17159</td>\n",
       "      <td>88</td>\n",
       "      <td>WCYY</td>\n",
       "      <td>[{'start': 84, 'end': 107, 'url': 'https://t.c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>@mikepompeo You think Stanford has rednecks th...</td>\n",
       "      <td>[1637881583066652672]</td>\n",
       "      <td>1637881583066652672</td>\n",
       "      <td>2023-03-20 18:19:34+00:00</td>\n",
       "      <td>1256022482643046400</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'username': 'mikepomp...</td>\n",
       "      <td>[{'start': 22, 'end': 29, 'probability': 0.731...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1256022482643046400</td>\n",
       "      <td>JT🌊</td>\n",
       "      <td>260</td>\n",
       "      <td>592</td>\n",
       "      <td>492</td>\n",
       "      <td>1</td>\n",
       "      <td>JThappadude</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>#空放   #天津  #兰州  #大连  Stanford Brown Walter Oni...</td>\n",
       "      <td>[1637881571901595648]</td>\n",
       "      <td>1637881571901595648</td>\n",
       "      <td>2023-03-20 18:19:31+00:00</td>\n",
       "      <td>2714020032</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2714020032</td>\n",
       "      <td>同城 约 啪 ·上海深圳成都苏州无锡南通盐城常州南京佛山东莞广州武汉青岛济南济宁潍坊温州老九...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>0</td>\n",
       "      <td>wessfd</td>\n",
       "      <td>[{'start': 63, 'end': 86, 'url': 'https://t.co...</td>\n",
       "      <td>[空放, 天津, 兰州, 大连]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>@ChrisCoble @RichardGrenell Can keep sending D...</td>\n",
       "      <td>[1637881523595558914]</td>\n",
       "      <td>1637881523595558912</td>\n",
       "      <td>2023-03-20 18:19:20+00:00</td>\n",
       "      <td>1179965994346917888</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'username': 'ChrisCob...</td>\n",
       "      <td>[{'start': 167, 'end': 178, 'probability': 0.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1179965994346917888</td>\n",
       "      <td>United &amp; Indivisable</td>\n",
       "      <td>74</td>\n",
       "      <td>396</td>\n",
       "      <td>12606</td>\n",
       "      <td>1</td>\n",
       "      <td>Happyboston1</td>\n",
       "      <td>[{'start': 218, 'end': 241, 'url': 'https://t....</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>@KeenanPeachy I’m going to tell my kid to writ...</td>\n",
       "      <td>[1637881455861702672]</td>\n",
       "      <td>1637881455861702656</td>\n",
       "      <td>2023-03-20 18:19:04+00:00</td>\n",
       "      <td>3956516777</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>[{'start': 0, 'end': 13, 'username': 'KeenanPe...</td>\n",
       "      <td>[{'start': 162, 'end': 169, 'probability': 0.8...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3956516777</td>\n",
       "      <td>Hossein Khorashadi</td>\n",
       "      <td>19</td>\n",
       "      <td>185</td>\n",
       "      <td>3436</td>\n",
       "      <td>1</td>\n",
       "      <td>hkhorashadi1</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>Jessie Rosa Stanford Spencer #郑州资源 #郑州 Giles C...</td>\n",
       "      <td>[1637881417958064128]</td>\n",
       "      <td>1637881417958064128</td>\n",
       "      <td>2023-03-20 18:18:55+00:00</td>\n",
       "      <td>1600174068786937856</td>\n",
       "      <td>Stanford</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1600174068786937856</td>\n",
       "      <td>Reo Hickingbotham</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3744</td>\n",
       "      <td>0</td>\n",
       "      <td>ReoHickingboth1</td>\n",
       "      <td>[{'start': 57, 'end': 80, 'url': 'https://t.co...</td>\n",
       "      <td>[郑州资源, 郑州]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     @RSIRob @scs_real He’s at Virginia Tech now. O...   \n",
       "1     Virginia Tech women's basketball advances to t...   \n",
       "2     The NCAA Women's Basketball Tournament banned ...   \n",
       "3     DaVinci Bead Virginia Tech – Jewelry Bracelet ...   \n",
       "4     Virginia Tech Fans Sing Metallica’s ‘Enter San...   \n",
       "...                                                 ...   \n",
       "2995  @mikepompeo You think Stanford has rednecks th...   \n",
       "2996  #空放   #天津  #兰州  #大连  Stanford Brown Walter Oni...   \n",
       "2997  @ChrisCoble @RichardGrenell Can keep sending D...   \n",
       "2998  @KeenanPeachy I’m going to tell my kid to writ...   \n",
       "2999  Jessie Rosa Stanford Spencer #郑州资源 #郑州 Giles C...   \n",
       "\n",
       "     edit_history_tweet_ids                   id                created_at  \\\n",
       "0     [1637901189865177096]  1637901189865177088 2023-03-20 19:37:29+00:00   \n",
       "1     [1637899898627448833]  1637899898627448832 2023-03-20 19:32:21+00:00   \n",
       "2     [1637899418279059458]  1637899418279059456 2023-03-20 19:30:26+00:00   \n",
       "3     [1637899291661598726]  1637899291661598720 2023-03-20 19:29:56+00:00   \n",
       "4     [1637898477525950464]  1637898477525950464 2023-03-20 19:26:42+00:00   \n",
       "...                     ...                  ...                       ...   \n",
       "2995  [1637881583066652672]  1637881583066652672 2023-03-20 18:19:34+00:00   \n",
       "2996  [1637881571901595648]  1637881571901595648 2023-03-20 18:19:31+00:00   \n",
       "2997  [1637881523595558914]  1637881523595558912 2023-03-20 18:19:20+00:00   \n",
       "2998  [1637881455861702672]  1637881455861702656 2023-03-20 18:19:04+00:00   \n",
       "2999  [1637881417958064128]  1637881417958064128 2023-03-20 18:18:55+00:00   \n",
       "\n",
       "                author_id            topic  \\\n",
       "0     1391907070031630336  \"Virginia Tech\"   \n",
       "1                45960001  \"Virginia Tech\"   \n",
       "2               237286926  \"Virginia Tech\"   \n",
       "3              3238156627  \"Virginia Tech\"   \n",
       "4                43113373  \"Virginia Tech\"   \n",
       "...                   ...              ...   \n",
       "2995  1256022482643046400         Stanford   \n",
       "2996           2714020032         Stanford   \n",
       "2997  1179965994346917888         Stanford   \n",
       "2998           3956516777         Stanford   \n",
       "2999  1600174068786937856         Stanford   \n",
       "\n",
       "                                      entities.mentions  \\\n",
       "0     [{'start': 0, 'end': 7, 'username': 'RSIRob', ...   \n",
       "1                                                  None   \n",
       "2                                                  None   \n",
       "3                                                  None   \n",
       "4                                                  None   \n",
       "...                                                 ...   \n",
       "2995  [{'start': 0, 'end': 11, 'username': 'mikepomp...   \n",
       "2996                                               None   \n",
       "2997  [{'start': 0, 'end': 11, 'username': 'ChrisCob...   \n",
       "2998  [{'start': 0, 'end': 13, 'username': 'KeenanPe...   \n",
       "2999                                               None   \n",
       "\n",
       "                                   entities.annotations  \\\n",
       "0     [{'start': 26, 'end': 38, 'probability': 0.979...   \n",
       "1     [{'start': 0, 'end': 18, 'probability': 0.8393...   \n",
       "2     [{'start': 4, 'end': 37, 'probability': 0.5539...   \n",
       "3                                                  None   \n",
       "4     [{'start': 0, 'end': 12, 'probability': 0.912,...   \n",
       "...                                                 ...   \n",
       "2995  [{'start': 22, 'end': 29, 'probability': 0.731...   \n",
       "2996                                               None   \n",
       "2997  [{'start': 167, 'end': 178, 'probability': 0.5...   \n",
       "2998  [{'start': 162, 'end': 169, 'probability': 0.8...   \n",
       "2999                                               None   \n",
       "\n",
       "      public_metrics.retweet_count  public_metrics.reply_count  ...  \\\n",
       "0                                0                           0  ...   \n",
       "1                                1                           0  ...   \n",
       "2                                0                           0  ...   \n",
       "3                                0                           0  ...   \n",
       "4                                0                           0  ...   \n",
       "...                            ...                         ...  ...   \n",
       "2995                             0                           0  ...   \n",
       "2996                             0                           0  ...   \n",
       "2997                             0                           0  ...   \n",
       "2998                             0                           0  ...   \n",
       "2999                             0                           0  ...   \n",
       "\n",
       "                author.id                                        author.name  \\\n",
       "0     1391907070031630336                        Grant Montgomery 2.0 🇺🇸🇮🇱🇯🇵   \n",
       "1                45960001                                   Collegiate Times   \n",
       "2               237286926                                          96.3 WROV   \n",
       "3              3238156627                                Wear Your Own Techs   \n",
       "4                43113373                                               WCYY   \n",
       "...                   ...                                                ...   \n",
       "2995  1256022482643046400                                                JT🌊   \n",
       "2996           2714020032  同城 约 啪 ·上海深圳成都苏州无锡南通盐城常州南京佛山东莞广州武汉青岛济南济宁潍坊温州老九...   \n",
       "2997  1179965994346917888                               United & Indivisable   \n",
       "2998           3956516777                                 Hossein Khorashadi   \n",
       "2999  1600174068786937856                                  Reo Hickingbotham   \n",
       "\n",
       "      author.public_metrics.followers_count  \\\n",
       "0                                      4508   \n",
       "1                                     20332   \n",
       "2                                       896   \n",
       "3                                      3684   \n",
       "4                                      3996   \n",
       "...                                     ...   \n",
       "2995                                    260   \n",
       "2996                                      0   \n",
       "2997                                     74   \n",
       "2998                                     19   \n",
       "2999                                      0   \n",
       "\n",
       "      author.public_metrics.following_count author.public_metrics.tweet_count  \\\n",
       "0                                      4510                             10791   \n",
       "1                                       327                             18488   \n",
       "2                                       529                             16209   \n",
       "3                                      3768                            139373   \n",
       "4                                       679                             17159   \n",
       "...                                     ...                               ...   \n",
       "2995                                    592                               492   \n",
       "2996                                      0                               490   \n",
       "2997                                    396                             12606   \n",
       "2998                                    185                              3436   \n",
       "2999                                      0                              3744   \n",
       "\n",
       "      author.public_metrics.listed_count  author.username  \\\n",
       "0                                      1      monsgomeric   \n",
       "1                                    448  CollegiateTimes   \n",
       "2                                      9         ROVRocks   \n",
       "3                                     36           wyot23   \n",
       "4                                     88             WCYY   \n",
       "...                                  ...              ...   \n",
       "2995                                   1      JThappadude   \n",
       "2996                                   0           wessfd   \n",
       "2997                                   1     Happyboston1   \n",
       "2998                                   1     hkhorashadi1   \n",
       "2999                                   0  ReoHickingboth1   \n",
       "\n",
       "                                          entities.urls  entities.hashtags  \\\n",
       "0                                                  None                 []   \n",
       "1     [{'start': 112, 'end': 135, 'url': 'https://t....                 []   \n",
       "2     [{'start': 136, 'end': 159, 'url': 'https://t....                 []   \n",
       "3     [{'start': 104, 'end': 127, 'url': 'https://t....                 []   \n",
       "4     [{'start': 84, 'end': 107, 'url': 'https://t.c...                 []   \n",
       "...                                                 ...                ...   \n",
       "2995                                               None                 []   \n",
       "2996  [{'start': 63, 'end': 86, 'url': 'https://t.co...   [空放, 天津, 兰州, 大连]   \n",
       "2997  [{'start': 218, 'end': 241, 'url': 'https://t....                 []   \n",
       "2998                                               None                 []   \n",
       "2999  [{'start': 57, 'end': 80, 'url': 'https://t.co...         [郑州资源, 郑州]   \n",
       "\n",
       "     entities.cashtags  \n",
       "0                 None  \n",
       "1                 None  \n",
       "2                 None  \n",
       "3                 None  \n",
       "4                 None  \n",
       "...                ...  \n",
       "2995              None  \n",
       "2996              None  \n",
       "2997              None  \n",
       "2998              None  \n",
       "2999              None  \n",
       "\n",
       "[3000 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = pd.read_json(\"tweets-lda.jsonl\", lines=True)\n",
    "\n",
    "#drop unneed metadata and turn it into a list, you can apply this to other listlike fields\n",
    "dt[\"entities.hashtags\"] = dt[\"entities.hashtags\"].apply(lambda r: [] if not r else r)\n",
    "dt[\"entities.hashtags\"] = dt[\"entities.hashtags\"].apply(lambda r: [h[\"tag\"] for h in r])\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>entities.hashtags</th>\n",
       "      <th>author.username</th>\n",
       "      <th>author.public_metrics.followers_count</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>2023-03-20 19:25:55+00:00</td>\n",
       "      <td>@INTPhilosopher @emrazz Harvard is asking the ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>79Merlot</td>\n",
       "      <td>344</td>\n",
       "      <td>Harvard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2023-03-20 18:59:53+00:00</td>\n",
       "      <td>Rory Margaret Caroline Tony #长沙资源 #长沙 Stanford...</td>\n",
       "      <td>[长沙资源, 长沙]</td>\n",
       "      <td>ChirafisiSummer</td>\n",
       "      <td>0</td>\n",
       "      <td>Stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2023-03-20 17:47:06+00:00</td>\n",
       "      <td>Ron graduated from Yale 💀🦴 &amp;amp; Harvard, and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>dade_only</td>\n",
       "      <td>55</td>\n",
       "      <td>Harvard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>2023-03-20 18:48:35+00:00</td>\n",
       "      <td>Bridget Bob Gale Ruskin Freda Grant Stanford M...</td>\n",
       "      <td>[布里斯托]</td>\n",
       "      <td>josiahpuglas_2</td>\n",
       "      <td>0</td>\n",
       "      <td>Stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>2023-03-19 23:22:48+00:00</td>\n",
       "      <td>Winner of Toledo-Lady Vols on Monday night wil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TeresaMWalker</td>\n",
       "      <td>14856</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>2023-03-20 19:24:36+00:00</td>\n",
       "      <td>@FNCOriginals @FoxNews No judges should accept...</td>\n",
       "      <td>[]</td>\n",
       "      <td>cyberiano42</td>\n",
       "      <td>2</td>\n",
       "      <td>Stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>2023-03-19 14:51:14+00:00</td>\n",
       "      <td>West Virginia and Virginia Tech are the latest...</td>\n",
       "      <td>[]</td>\n",
       "      <td>jakeweingarten</td>\n",
       "      <td>34510</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-03-20 18:53:01+00:00</td>\n",
       "      <td>Merrimack transfer Ziggy Reid tells TPR that h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>ThePortalReport</td>\n",
       "      <td>10086</td>\n",
       "      <td>\"Virginia Tech\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>2023-03-20 19:24:17+00:00</td>\n",
       "      <td>Cedric Child Muriel Sheridan #青岛资源 #青岛 Stanfor...</td>\n",
       "      <td>[青岛资源, 青岛]</td>\n",
       "      <td>CottinghamRoss</td>\n",
       "      <td>0</td>\n",
       "      <td>Stanford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>2023-03-20 18:15:03+00:00</td>\n",
       "      <td>With the Big Ten and PWR as tight as they were...</td>\n",
       "      <td>[]</td>\n",
       "      <td>YostBuilt</td>\n",
       "      <td>3260</td>\n",
       "      <td>Harvard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    created_at  \\\n",
       "1064 2023-03-20 19:25:55+00:00   \n",
       "2534 2023-03-20 18:59:53+00:00   \n",
       "1438 2023-03-20 17:47:06+00:00   \n",
       "2679 2023-03-20 18:48:35+00:00   \n",
       "346  2023-03-19 23:22:48+00:00   \n",
       "2209 2023-03-20 19:24:36+00:00   \n",
       "605  2023-03-19 14:51:14+00:00   \n",
       "14   2023-03-20 18:53:01+00:00   \n",
       "2214 2023-03-20 19:24:17+00:00   \n",
       "1338 2023-03-20 18:15:03+00:00   \n",
       "\n",
       "                                                   text entities.hashtags  \\\n",
       "1064  @INTPhilosopher @emrazz Harvard is asking the ...                []   \n",
       "2534  Rory Margaret Caroline Tony #长沙资源 #长沙 Stanford...        [长沙资源, 长沙]   \n",
       "1438  Ron graduated from Yale 💀🦴 &amp; Harvard, and ...                []   \n",
       "2679  Bridget Bob Gale Ruskin Freda Grant Stanford M...            [布里斯托]   \n",
       "346   Winner of Toledo-Lady Vols on Monday night wil...                []   \n",
       "2209  @FNCOriginals @FoxNews No judges should accept...                []   \n",
       "605   West Virginia and Virginia Tech are the latest...                []   \n",
       "14    Merrimack transfer Ziggy Reid tells TPR that h...                []   \n",
       "2214  Cedric Child Muriel Sheridan #青岛资源 #青岛 Stanfor...        [青岛资源, 青岛]   \n",
       "1338  With the Big Ten and PWR as tight as they were...                []   \n",
       "\n",
       "      author.username  author.public_metrics.followers_count            topic  \n",
       "1064         79Merlot                                    344          Harvard  \n",
       "2534  ChirafisiSummer                                      0         Stanford  \n",
       "1438        dade_only                                     55          Harvard  \n",
       "2679   josiahpuglas_2                                      0         Stanford  \n",
       "346     TeresaMWalker                                  14856  \"Virginia Tech\"  \n",
       "2209      cyberiano42                                      2         Stanford  \n",
       "605    jakeweingarten                                  34510  \"Virginia Tech\"  \n",
       "14    ThePortalReport                                  10086  \"Virginia Tech\"  \n",
       "2214   CottinghamRoss                                      0         Stanford  \n",
       "1338        YostBuilt                                   3260          Harvard  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[['created_at','text','entities.hashtags','author.username','author.public_metrics.followers_count','topic']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Virginia Tech women's basketball advances to the Sweet 16 of the NCAA Tournament for the first time since 1999.\\nhttps://t.co/mi4tyVYRPI\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[\"text\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing only the text of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@RSIRob @scs_real He’s at Virginia Tech now. Ohio State didn’t work out.\n"
     ]
    }
   ],
   "source": [
    "all_docs = dt['text'].values\n",
    "print(all_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization using TweetTokenizer\n",
    "This tokenizer is customized for tokenizing tweet data. Try using a different tokenizer to see how the result of your LDA will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virginia', 'tech', 'womens', 'basketball', 'advances', 'to', 'the', 'sweet', '16', 'of', 'the', 'ncaa', 'tournament', 'for', 'the', 'first', 'time', 'since', '1999', 'httpstcomi4tyvyrpi']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "tokenized = []\n",
    "tokenizer = TweetTokenizer()\n",
    "for doc in all_docs:\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    tokenized.append(''.join([ch for ch in ' '.join(tokens) if ch not in exclude]).split())\n",
    "print(tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal\n",
    "Certain parts of English speech, like conjunctions (\"in\", \"for\") are meaningless to a topic model. These terms are called stop words and need to be removed from our token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virginia', 'tech', 'womens', 'basketball', 'advances', 'sweet', 'ncaa', 'tournament', 'first', 'time', 'since', '1999']\n"
     ]
    }
   ],
   "source": [
    "sws = set(stopwords.words('english'))\n",
    "sws.add('rt') # Tweet specific stop-words\n",
    "sws.add(\"…\") \n",
    "sws_removed = []\n",
    "for j,sent in enumerate(tokenized):\n",
    "    sws_removed.append([i for i in sent \n",
    "                        if i not in sws   # drop stopwords \n",
    "                             and len(i) > 2  # drop words of insig length\n",
    "                             and (not i.startswith(\"http\"))])  #drop links\n",
    "print(sws_removed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Library\n",
    "The result of our cleaning stage is tweets, a tokenized, stop-removed list of words from a single tweet. We looped through all our documents and appended each one to our sws_removed variable. So now sws_removed is a list of lists, one list for each of our original tweets.\n",
    "\n",
    "To generate an LDA model, we need to understand how frequently each term occurs within each document. To do that, we need to construct a document-term matrix with a package called gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(sws_removed)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dictionary() function traverses data, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics. To see each token's unique integer id, try print(dictionary.token2id).\n",
    "\n",
    "Next, our dictionary must be converted into a bag-of-words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in sws_removed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The doc2bow() function converts dictionary into a bag-of-words. The result, corpus, is a list of vectors equal to the number of tweets. In each tweet vector is a series of tuples. As an example, print(corpus[2]) results in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of tuples represents our first tweet. The tuples are (term ID, term frequency) pairs. doc2bow() only includes terms that actually occur: terms that do not occur in a tweet will not appear in that tweet's vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA model\n",
    "corpus is a document-term matrix and now we are ready to generate an LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LdaModel class is described in detail in the gensim documentation.\n",
    "Parameters used in our example:\n",
    "num_topics: required. An LDA model requires the user to determine how many topics should be generated. Our document set is small, so we’re only asking for three topics.\n",
    "id2word: required. The LdaModel class requires our previous dictionary to map ids to strings.\n",
    "passes: optional. The number of laps the model will take through corpus. The greater the number of passes, the more accurate the model will be. A lot of passes can be slow on a very large corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the results\n",
    "Our LDA model is now stored as ldamodel. We can review our topics with the print_topic and print_topics methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"tech\" + 0.031*\"virginia\" + 0.019*\"state\" + 0.009*\"new\" + 0.007*\"college\"'),\n",
       " (1,\n",
       "  '0.043*\"virginia\" + 0.043*\"tech\" + 0.013*\"game\" + 0.012*\"ncaa\" + 0.009*\"hokies\"'),\n",
       " (2,\n",
       "  '0.048*\"harvard\" + 0.013*\"law\" + 0.009*\"brown\" + 0.009*\"like\" + 0.008*\"school\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=5, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean? Each generated topic is separated by a comma. Within each topic are the five most probable words to appear in that topic. Even though our document set is small the model is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.71576947), (2, 0.21987227), (1, 0.064358294)]: rsirob scsreal virginia tech ohio state work\n",
      "\n",
      "[(1, 0.48604637), (0, 0.42221296), (2, 0.09174066)]: elizabeth kitley currently tied ieva kublina hokies alltime blocks leader 256 needs one break virginia tech 551 nonconference foes cassell coliseum kenny brooks techs seeking secondever sweet berth 1999\n",
      "\n",
      "[(1, 0.7421453), (2, 0.18004228), (0, 0.077812426)]: harvard grime release pro carpet cleaning prespray traffic lane cleaner case ebay\n",
      "\n",
      "[(2, 0.9382218), (0, 0.031068841), (1, 0.030709373)]: excellent possible explanation desantiss actions statement regarding president trump would navy jag harvard guy ron parttime bona fide fed\n",
      "\n",
      "[(1, 0.9030132), (2, 0.04933226), (0, 0.047654513)]: stanford arthur vito haydn 青岛资源 aaron london\n",
      "\n",
      "[(0, 0.903958), (2, 0.048402734), (1, 0.047639202)]: louise harrison stanford smith 成都旅游 raymond grote\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(corpus), 500):  \n",
    "    topics = ldamodel.get_document_topics(corpus[i])\n",
    "    topics = sorted(topics, key=lambda x: -x[1])\n",
    "    print(\"{}: {}\\n\".format(topics, \" \".join(sws_removed[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try with two topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.038*\"harvard\" + 0.012*\"law\" + 0.009*\"school\" + 0.008*\"university\" + 0.007*\"like\"'),\n",
       " (1,\n",
       "  '0.049*\"virginia\" + 0.049*\"tech\" + 0.014*\"state\" + 0.011*\"ncaa\" + 0.011*\"hokies\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=2, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
